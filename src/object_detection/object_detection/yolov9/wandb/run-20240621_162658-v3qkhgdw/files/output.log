Overriding model.yaml nc=80 with nc=1
                 from  n    params  module                                  arguments
  0                -1  1         0  models.common.Silence                   []
  1                -1  1      1856  models.common.Conv                      [3, 64, 3, 2]
  2                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]
  3                -1  1    212864  models.common.RepNCSPELAN4              [128, 256, 128, 64, 1]
  4                -1  1    164352  models.common.ADown                     [256, 256]
  5                -1  1    847616  models.common.RepNCSPELAN4              [256, 512, 256, 128, 1]
  6                -1  1    656384  models.common.ADown                     [512, 512]
  7                -1  1   2857472  models.common.RepNCSPELAN4              [512, 512, 512, 256, 1]
  8                -1  1    656384  models.common.ADown                     [512, 512]
  9                -1  1   2857472  models.common.RepNCSPELAN4              [512, 512, 512, 256, 1]
 10                -1  1    656896  models.common.SPPELAN                   [512, 512, 256]
 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']
 12           [-1, 7]  1         0  models.common.Concat                    [1]
 13                -1  1   3119616  models.common.RepNCSPELAN4              [1024, 512, 512, 256, 1]
 14                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']
 15           [-1, 5]  1         0  models.common.Concat                    [1]
 16                -1  1    912640  models.common.RepNCSPELAN4              [1024, 256, 256, 128, 1]
 17                -1  1    164352  models.common.ADown                     [256, 256]
 18          [-1, 13]  1         0  models.common.Concat                    [1]
 19                -1  1   2988544  models.common.RepNCSPELAN4              [768, 512, 512, 256, 1]
 20                -1  1    656384  models.common.ADown                     [512, 512]
 21          [-1, 10]  1         0  models.common.Concat                    [1]
 22                -1  1   3119616  models.common.RepNCSPELAN4              [1024, 512, 512, 256, 1]
 23                 5  1    131328  models.common.CBLinear                  [512, [256]]
 24                 7  1    393984  models.common.CBLinear                  [512, [256, 512]]
 25                 9  1    656640  models.common.CBLinear                  [512, [256, 512, 512]]
 26                 0  1      1856  models.common.Conv                      [3, 64, 3, 2]
 27                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]
 28                -1  1    212864  models.common.RepNCSPELAN4              [128, 256, 128, 64, 1]
 29                -1  1    164352  models.common.ADown                     [256, 256]
 30  [23, 24, 25, -1]  1         0  models.common.CBFuse                    [[0, 0, 0]]
 31                -1  1    847616  models.common.RepNCSPELAN4              [256, 512, 256, 128, 1]
 32                -1  1    656384  models.common.ADown                     [512, 512]
 33      [24, 25, -1]  1         0  models.common.CBFuse                    [[1, 1]]
 34                -1  1   2857472  models.common.RepNCSPELAN4              [512, 512, 512, 256, 1]
 35                -1  1    656384  models.common.ADown                     [512, 512]
 36          [25, -1]  1         0  models.common.CBFuse                    [[2]]
 37                -1  1   2857472  models.common.RepNCSPELAN4              [512, 512, 512, 256, 1]
 38[31, 34, 37, 16, 19, 22]  1  21542822  models.yolo.DualDDetect                 [1, [512, 512, 512, 256, 512, 512]]
yolov9-c summary: 962 layers, 50999590 parameters, 50999558 gradients, 238.9 GFLOPs
Transferred 1448/1460 items from weights/yolov9-c.pt
[34m[1mAMP: [39m[22mchecks passed ✅
freezing model.1.conv.weight
freezing model.1.bn.weight
freezing model.1.bn.bias
freezing model.2.conv.weight
freezing model.2.bn.weight
freezing model.2.bn.bias
freezing model.3.cv1.conv.weight
freezing model.3.cv1.bn.weight
freezing model.3.cv1.bn.bias
freezing model.3.cv2.0.cv1.conv.weight
freezing model.3.cv2.0.cv1.bn.weight
freezing model.3.cv2.0.cv1.bn.bias
freezing model.3.cv2.0.cv2.conv.weight
freezing model.3.cv2.0.cv2.bn.weight
freezing model.3.cv2.0.cv2.bn.bias
freezing model.3.cv2.0.cv3.conv.weight
freezing model.3.cv2.0.cv3.bn.weight
freezing model.3.cv2.0.cv3.bn.bias
freezing model.3.cv2.0.m.0.cv1.conv1.conv.weight
freezing model.3.cv2.0.m.0.cv1.conv1.bn.weight
freezing model.3.cv2.0.m.0.cv1.conv1.bn.bias
freezing model.3.cv2.0.m.0.cv1.conv2.conv.weight
freezing model.3.cv2.0.m.0.cv1.conv2.bn.weight
freezing model.3.cv2.0.m.0.cv1.conv2.bn.bias
freezing model.3.cv2.0.m.0.cv2.conv.weight
freezing model.3.cv2.0.m.0.cv2.bn.weight
freezing model.3.cv2.0.m.0.cv2.bn.bias
freezing model.3.cv2.1.conv.weight
freezing model.3.cv2.1.bn.weight
freezing model.3.cv2.1.bn.bias
freezing model.3.cv3.0.cv1.conv.weight
freezing model.3.cv3.0.cv1.bn.weight
freezing model.3.cv3.0.cv1.bn.bias
freezing model.3.cv3.0.cv2.conv.weight
freezing model.3.cv3.0.cv2.bn.weight
freezing model.3.cv3.0.cv2.bn.bias
freezing model.3.cv3.0.cv3.conv.weight
freezing model.3.cv3.0.cv3.bn.weight
freezing model.3.cv3.0.cv3.bn.bias
freezing model.3.cv3.0.m.0.cv1.conv1.conv.weight
freezing model.3.cv3.0.m.0.cv1.conv1.bn.weight
freezing model.3.cv3.0.m.0.cv1.conv1.bn.bias
freezing model.3.cv3.0.m.0.cv1.conv2.conv.weight
freezing model.3.cv3.0.m.0.cv1.conv2.bn.weight
freezing model.3.cv3.0.m.0.cv1.conv2.bn.bias
freezing model.3.cv3.0.m.0.cv2.conv.weight
freezing model.3.cv3.0.m.0.cv2.bn.weight
freezing model.3.cv3.0.m.0.cv2.bn.bias
freezing model.3.cv3.1.conv.weight
freezing model.3.cv3.1.bn.weight
freezing model.3.cv3.1.bn.bias
freezing model.3.cv4.conv.weight
freezing model.3.cv4.bn.weight
freezing model.3.cv4.bn.bias
freezing model.4.cv1.conv.weight
freezing model.4.cv1.bn.weight
freezing model.4.cv1.bn.bias
freezing model.4.cv2.conv.weight
freezing model.4.cv2.bn.weight
freezing model.4.cv2.bn.bias
freezing model.5.cv1.conv.weight
freezing model.5.cv1.bn.weight
freezing model.5.cv1.bn.bias
freezing model.5.cv2.0.cv1.conv.weight
freezing model.5.cv2.0.cv1.bn.weight
freezing model.5.cv2.0.cv1.bn.bias
freezing model.5.cv2.0.cv2.conv.weight
freezing model.5.cv2.0.cv2.bn.weight
freezing model.5.cv2.0.cv2.bn.bias
freezing model.5.cv2.0.cv3.conv.weight
freezing model.5.cv2.0.cv3.bn.weight
freezing model.5.cv2.0.cv3.bn.bias
freezing model.5.cv2.0.m.0.cv1.conv1.conv.weight
freezing model.5.cv2.0.m.0.cv1.conv1.bn.weight
freezing model.5.cv2.0.m.0.cv1.conv1.bn.bias
freezing model.5.cv2.0.m.0.cv1.conv2.conv.weight
freezing model.5.cv2.0.m.0.cv1.conv2.bn.weight
freezing model.5.cv2.0.m.0.cv1.conv2.bn.bias
freezing model.5.cv2.0.m.0.cv2.conv.weight
freezing model.5.cv2.0.m.0.cv2.bn.weight
freezing model.5.cv2.0.m.0.cv2.bn.bias
freezing model.5.cv2.1.conv.weight
freezing model.5.cv2.1.bn.weight
freezing model.5.cv2.1.bn.bias
freezing model.5.cv3.0.cv1.conv.weight
freezing model.5.cv3.0.cv1.bn.weight
freezing model.5.cv3.0.cv1.bn.bias
freezing model.5.cv3.0.cv2.conv.weight
freezing model.5.cv3.0.cv2.bn.weight
freezing model.5.cv3.0.cv2.bn.bias
freezing model.5.cv3.0.cv3.conv.weight
freezing model.5.cv3.0.cv3.bn.weight
freezing model.5.cv3.0.cv3.bn.bias
freezing model.5.cv3.0.m.0.cv1.conv1.conv.weight
freezing model.5.cv3.0.m.0.cv1.conv1.bn.weight
freezing model.5.cv3.0.m.0.cv1.conv1.bn.bias
freezing model.5.cv3.0.m.0.cv1.conv2.conv.weight
freezing model.5.cv3.0.m.0.cv1.conv2.bn.weight
freezing model.5.cv3.0.m.0.cv1.conv2.bn.bias
freezing model.5.cv3.0.m.0.cv2.conv.weight
freezing model.5.cv3.0.m.0.cv2.bn.weight
freezing model.5.cv3.0.m.0.cv2.bn.bias
freezing model.5.cv3.1.conv.weight
freezing model.5.cv3.1.bn.weight
freezing model.5.cv3.1.bn.bias
freezing model.5.cv4.conv.weight
freezing model.5.cv4.bn.weight
freezing model.5.cv4.bn.bias
freezing model.6.cv1.conv.weight
freezing model.6.cv1.bn.weight
freezing model.6.cv1.bn.bias
freezing model.6.cv2.conv.weight
freezing model.6.cv2.bn.weight
freezing model.6.cv2.bn.bias
freezing model.7.cv1.conv.weight
freezing model.7.cv1.bn.weight
freezing model.7.cv1.bn.bias
freezing model.7.cv2.0.cv1.conv.weight
freezing model.7.cv2.0.cv1.bn.weight
freezing model.7.cv2.0.cv1.bn.bias
freezing model.7.cv2.0.cv2.conv.weight
freezing model.7.cv2.0.cv2.bn.weight
freezing model.7.cv2.0.cv2.bn.bias
freezing model.7.cv2.0.cv3.conv.weight
freezing model.7.cv2.0.cv3.bn.weight
freezing model.7.cv2.0.cv3.bn.bias
freezing model.7.cv2.0.m.0.cv1.conv1.conv.weight
freezing model.7.cv2.0.m.0.cv1.conv1.bn.weight
freezing model.7.cv2.0.m.0.cv1.conv1.bn.bias
freezing model.7.cv2.0.m.0.cv1.conv2.conv.weight
freezing model.7.cv2.0.m.0.cv1.conv2.bn.weight
freezing model.7.cv2.0.m.0.cv1.conv2.bn.bias
freezing model.7.cv2.0.m.0.cv2.conv.weight
freezing model.7.cv2.0.m.0.cv2.bn.weight
freezing model.7.cv2.0.m.0.cv2.bn.bias
freezing model.7.cv2.1.conv.weight
freezing model.7.cv2.1.bn.weight
freezing model.7.cv2.1.bn.bias
freezing model.7.cv3.0.cv1.conv.weight
freezing model.7.cv3.0.cv1.bn.weight
freezing model.7.cv3.0.cv1.bn.bias
freezing model.7.cv3.0.cv2.conv.weight
freezing model.7.cv3.0.cv2.bn.weight
freezing model.7.cv3.0.cv2.bn.bias
freezing model.7.cv3.0.cv3.conv.weight
freezing model.7.cv3.0.cv3.bn.weight
freezing model.7.cv3.0.cv3.bn.bias
freezing model.7.cv3.0.m.0.cv1.conv1.conv.weight
freezing model.7.cv3.0.m.0.cv1.conv1.bn.weight
freezing model.7.cv3.0.m.0.cv1.conv1.bn.bias
freezing model.7.cv3.0.m.0.cv1.conv2.conv.weight
freezing model.7.cv3.0.m.0.cv1.conv2.bn.weight
freezing model.7.cv3.0.m.0.cv1.conv2.bn.bias
freezing model.7.cv3.0.m.0.cv2.conv.weight
freezing model.7.cv3.0.m.0.cv2.bn.weight
freezing model.7.cv3.0.m.0.cv2.bn.bias
freezing model.7.cv3.1.conv.weight
freezing model.7.cv3.1.bn.weight
freezing model.7.cv3.1.bn.bias
freezing model.7.cv4.conv.weight
freezing model.7.cv4.bn.weight
freezing model.7.cv4.bn.bias
freezing model.8.cv1.conv.weight
freezing model.8.cv1.bn.weight
freezing model.8.cv1.bn.bias
freezing model.8.cv2.conv.weight
freezing model.8.cv2.bn.weight
freezing model.8.cv2.bn.bias
freezing model.9.cv1.conv.weight
freezing model.9.cv1.bn.weight
freezing model.9.cv1.bn.bias
freezing model.9.cv2.0.cv1.conv.weight
freezing model.9.cv2.0.cv1.bn.weight
freezing model.9.cv2.0.cv1.bn.bias
freezing model.9.cv2.0.cv2.conv.weight
freezing model.9.cv2.0.cv2.bn.weight
freezing model.9.cv2.0.cv2.bn.bias
freezing model.9.cv2.0.cv3.conv.weight
freezing model.9.cv2.0.cv3.bn.weight
freezing model.9.cv2.0.cv3.bn.bias
freezing model.9.cv2.0.m.0.cv1.conv1.conv.weight
freezing model.9.cv2.0.m.0.cv1.conv1.bn.weight
freezing model.9.cv2.0.m.0.cv1.conv1.bn.bias
freezing model.9.cv2.0.m.0.cv1.conv2.conv.weight
freezing model.9.cv2.0.m.0.cv1.conv2.bn.weight
freezing model.9.cv2.0.m.0.cv1.conv2.bn.bias
freezing model.9.cv2.0.m.0.cv2.conv.weight
freezing model.9.cv2.0.m.0.cv2.bn.weight
freezing model.9.cv2.0.m.0.cv2.bn.bias
freezing model.9.cv2.1.conv.weight
freezing model.9.cv2.1.bn.weight
freezing model.9.cv2.1.bn.bias
freezing model.9.cv3.0.cv1.conv.weight
freezing model.9.cv3.0.cv1.bn.weight
freezing model.9.cv3.0.cv1.bn.bias
freezing model.9.cv3.0.cv2.conv.weight
freezing model.9.cv3.0.cv2.bn.weight
freezing model.9.cv3.0.cv2.bn.bias
freezing model.9.cv3.0.cv3.conv.weight
freezing model.9.cv3.0.cv3.bn.weight
freezing model.9.cv3.0.cv3.bn.bias
freezing model.9.cv3.0.m.0.cv1.conv1.conv.weight
freezing model.9.cv3.0.m.0.cv1.conv1.bn.weight
freezing model.9.cv3.0.m.0.cv1.conv1.bn.bias
freezing model.9.cv3.0.m.0.cv1.conv2.conv.weight
freezing model.9.cv3.0.m.0.cv1.conv2.bn.weight
freezing model.9.cv3.0.m.0.cv1.conv2.bn.bias
freezing model.9.cv3.0.m.0.cv2.conv.weight
freezing model.9.cv3.0.m.0.cv2.bn.weight
freezing model.9.cv3.0.m.0.cv2.bn.bias
freezing model.9.cv3.1.conv.weight
freezing model.9.cv3.1.bn.weight
freezing model.9.cv3.1.bn.bias
freezing model.9.cv4.conv.weight
freezing model.9.cv4.bn.weight
freezing model.9.cv4.bn.bias
[34m[1moptimizer:[39m[22m SGD(lr=0.003) with parameter groups 238 weight(decay=0.0), 255 weight(decay=0.0005), 253 bias
[34m[1malbumentations: [39m[22mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))
[34m[1mtrain: [39m[22mScanning /home/animated/Projects/Sailbot/src/object_detection/object_detection/yolov9/SailbotVT-3/train/labels.cache... 8130 images, 0 backgrounds, 0
[34m[1mval: [39m[22mScanning /home/animated/Projects/Sailbot/src/object_detection/object_detection/yolov9/SailbotVT-3/valid/labels.cache... 765 images, 0 backgrounds, 0 co
Plotting labels to runs/train/exp51/labels.jpg...
Image sizes 640 train, 640 val
Using 4 dataloader workers
Logging results to [1mruns/train/exp51
Starting training for 4 epochs...
      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
        0/3      3.44G      3.141      8.887      2.418          4        640:   0%|          | 0/2033 00:01Exception in thread Thread-8 (plot_images):
Traceback (most recent call last):
  File "/home/animated/miniforge3/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/home/animated/miniforge3/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/home/animated/Projects/Sailbot/src/object_detection/object_detection/yolov9/utils/plots.py", line 300, in plot_images
    annotator.box_label(box, label, color=color)
  File "/home/animated/Projects/Sailbot/src/object_detection/object_detection/yolov9/utils/plots.py", line 86, in box_label
    w, h = self.font.getsize(label)  # text width, height
AttributeError: 'FreeTypeFont' object has no attribute 'getsize'
WARNING ⚠️ TensorBoard graph visualization failure Only tensors, lists, tuples of tensors, or dictionary of tensors can be output from traced functions
        0/3      3.46G      3.477      8.861      2.665          4        640:   0%|          | 2/2033 00:03Exception in thread Thread-9 (plot_images):
Traceback (most recent call last):
  File "/home/animated/miniforge3/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/home/animated/miniforge3/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/home/animated/Projects/Sailbot/src/object_detection/object_detection/yolov9/utils/plots.py", line 300, in plot_images
    annotator.box_label(box, label, color=color)
  File "/home/animated/Projects/Sailbot/src/object_detection/object_detection/yolov9/utils/plots.py", line 86, in box_label
    w, h = self.font.getsize(label)  # text width, height
AttributeError: 'FreeTypeFont' object has no attribute 'getsize'
        0/3      3.46G      3.142      9.495      2.433          4        640:   0%|          | 3/2033 00:03Exception in thread Thread-10 (plot_images):
Traceback (most recent call last):
  File "/home/animated/miniforge3/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/home/animated/miniforge3/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/home/animated/Projects/Sailbot/src/object_detection/object_detection/yolov9/utils/plots.py", line 300, in plot_images
    annotator.box_label(box, label, color=color)
  File "/home/animated/Projects/Sailbot/src/object_detection/object_detection/yolov9/utils/plots.py", line 86, in box_label
    w, h = self.font.getsize(label)  # text width, height
AttributeError: 'FreeTypeFont' object has no attribute 'getsize'










        0/3      3.78G      2.875      6.677      2.401          4        640:   4%|▍         | 87/2033 00:26
Traceback (most recent call last):
  File "/home/animated/Projects/Sailbot/src/object_detection/object_detection/yolov9/train_dual.py", line 644, in <module>
    main(opt)
  File "/home/animated/Projects/Sailbot/src/object_detection/object_detection/yolov9/train_dual.py", line 538, in main
    train(opt.hyp, opt, device, callbacks)
  File "/home/animated/Projects/Sailbot/src/object_detection/object_detection/yolov9/train_dual.py", line 332, in train
    ema.update(model)
  File "/home/animated/Projects/Sailbot/src/object_detection/object_detection/yolov9/utils/torch_utils.py", line 520, in update
    msd = de_parallel(model).state_dict()  # model state_dict
  File "/home/animated/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1916, in state_dict
    module.state_dict(destination=destination, prefix=prefix + name + '.', keep_vars=keep_vars)
  File "/home/animated/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1916, in state_dict
    module.state_dict(destination=destination, prefix=prefix + name + '.', keep_vars=keep_vars)
  File "/home/animated/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1916, in state_dict
    module.state_dict(destination=destination, prefix=prefix + name + '.', keep_vars=keep_vars)
  [Previous line repeated 2 more times]
  File "/home/animated/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in state_dict
    def state_dict(self, *args, destination=None, prefix='', keep_vars=False):
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/animated/Projects/Sailbot/src/object_detection/object_detection/yolov9/train_dual.py", line 644, in <module>
    main(opt)
  File "/home/animated/Projects/Sailbot/src/object_detection/object_detection/yolov9/train_dual.py", line 538, in main
    train(opt.hyp, opt, device, callbacks)
  File "/home/animated/Projects/Sailbot/src/object_detection/object_detection/yolov9/train_dual.py", line 332, in train
    ema.update(model)
  File "/home/animated/Projects/Sailbot/src/object_detection/object_detection/yolov9/utils/torch_utils.py", line 520, in update
    msd = de_parallel(model).state_dict()  # model state_dict
  File "/home/animated/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1916, in state_dict
    module.state_dict(destination=destination, prefix=prefix + name + '.', keep_vars=keep_vars)
  File "/home/animated/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1916, in state_dict
    module.state_dict(destination=destination, prefix=prefix + name + '.', keep_vars=keep_vars)
  File "/home/animated/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1916, in state_dict
    module.state_dict(destination=destination, prefix=prefix + name + '.', keep_vars=keep_vars)
  [Previous line repeated 2 more times]
  File "/home/animated/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1845, in state_dict
    def state_dict(self, *args, destination=None, prefix='', keep_vars=False):
KeyboardInterrupt